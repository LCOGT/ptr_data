org: photonadmin
app: photonranch

service: ptrdata

plugins:
  - serverless-dotenv-plugin
  - serverless-plugin-existing-s3
  - serverless-python-requirements

package:
  patterns:
    - '!venv/**'
    - '!node_modules/**'
    - '!notebooks/**'
    - '!__pycache__/**'
    - '!.pytest_cache/**'
    - '!.env'
    - '!database.ini'
    - '!lambda_tests.txt'
    - '!lambda_service/tests/**'

custom: 
  # This is to reduce the size of the deployment
  pythonRequirements:
    dockerizePip: non-linux
    dockerFile: serverless-deploy-helpers/Dockerfile
    strip: false
    slim: true
    noDeploy:
      - pytest
      - pytest-env

provider:
  name: aws
  region: us-east-1
  runtime: python3.7
  lambdaHashingVersion: 20201221
  environment: 
    REGION: ${self:provider.region}
    BUCKET_NAME: photonranch-001
    EXPIRATION_TABLE: data-expiration-tracker
    INFO_IMAGES_TABLE: info-images
  iam:
    role:
      name: ptrdata-default-iam-role
      statements:
        - Effect: Allow
          Action:
            - s3:GetBucketNotification
            - s3:PutBucketNotification
            - s3:ListBucket
            - s3:DeleteObject
            - s3:GetObject  
            - s3:PutObject
          Resource: 
            - "arn:aws:s3:::${self:provider.environment.BUCKET_NAME}"
            - "arn:aws:s3:::${self:provider.environment.BUCKET_NAME}/*"

        - Effect: Allow
          Action:
            - ssm:GetParameter
          Resource: "arn:aws:ssm:${self:provider.region}:*:parameter/*"

        - Effect: Allow 
          Action: 
            - dynamodb:PutItem
            - dynamodb:GetItem
            - dynamodb:UpdateItem
            - dynamodb:DeleteItem
            - dynamodb:BatchGetItem
            - dynamodb:BatchWriteItem
            - dynamodb:Scan
            - dynamodb:Query
            - dynamodb:DescribeStream
            - dynamodb:GetRecords
            - dynamodb:GetShardIterator
            - dynamodb:ListStreams
          Resource:
            Fn::Join:
              - ""
              - - "arn:aws:dynamodb:"
                - ${self:provider.region}
                - ":*:*"

        - Effect: Allow
          Action:
            - sqs:SendMessage
            - sqs:GetQueueUrl
          Resource:
            - "arn:aws:sqs:${self:provider.region}:*:*"


functions:

  insert_data:
    handler: lambda_service/insert_data.handle_s3_object_created
    layers:
      - arn:aws:lambda:us-east-1:770693421928:layer:Klayers-python38-SQLAlchemy:18
      - arn:aws:lambda:us-east-1:770693421928:layer:Klayers-python38-Pillow:10
    events:
      - s3:
          bucket: ${self:provider.environment.BUCKET_NAME}
          event: s3:ObjectCreated:* 
          rules: 
            - prefix: data/
          existing: true
  insertInfoImage:
    handler: lambda_service/info_images.handle_info_image_created
    events:
      - s3:
          bucket: ${self:provider.environment.BUCKET_NAME}
          event: s3:ObjectCreated:* 
          rules: 
            - prefix: info-images/
          existing: true

  removeExpiredData:
    handler: lambda_service/expirations.remove_expired_data_handler
    events:
      - stream: 
          type: dynamodb
          batchSize: 1
          arn:
            Fn::GetAtt:
              - dataExpirationTracker
              - StreamArn

resources:
  Resources:

    dataExpirationTracker: 
      Type: AWS::DynamoDB::Table
      Properties:
        TableName: ${self:provider.environment.EXPIRATION_TABLE}
        AttributeDefinitions:
          - AttributeName: pk
            AttributeType: S
        KeySchema:
          - AttributeName: pk
            KeyType: HASH
        ProvisionedThroughput:
          ReadCapacityUnits: 1
          WriteCapacityUnits: 1
        TimeToLiveSpecification:
          AttributeName: expiration_timestamp_s
          Enabled: true
        StreamSpecification: 
          StreamViewType: NEW_AND_OLD_IMAGES
